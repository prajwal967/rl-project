{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP DETERMINISTIC POLICY GRADIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RL agent interacts with its environment, takes an action, and learns more about the environment, influencing subsequent actions and learning. Like an RL agent, we will first take an action that takes us into the Policy Gradient state. Once we have a basic understanding of the policy gradient environment, we take our next action, which leads us to the Deterministic Policy Gradients state. We improve our learning further in this state. We finally take the action that leads us to our goal state, i.e., Deep Deterministic Policy Gradients.\n",
    "\n",
    "**Let's head to our start state - Policy Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Definition\n",
    "Policy gradient methods learn a parameterized policy that can select actions without consulting a value function. They model and optimize the policy directly.\n",
    "\n",
    "$$\\pi(a|s, \\theta)=Pr\\{A_{t} = a\\ | S_{t} = s, \\theta_{t} = \\theta\\}$$\n",
    "\n",
    "This equation denotes the probability that action a is taken at time t given that the environment is in state s at time t with parameter $\\theta$.\n",
    "\n",
    "An example of a policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Reminds you of the softmax equation we so often see in machine learning, doesn't it?\n",
    "We call this kind of policy parameterization as as *softmax in action preferences*.\n",
    "Like in machine learning we would have $h(s, a, \\theta) = \\theta^T x(s, a)$ where $x(s, a)$ is a feature vector and $\\theta$ are the weights.\n",
    "\n",
    "**Now that we've covered the *Policy* portion, let's move on to the *Gradient* portion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Policy Gradient Theorem\n",
    "Let's define our expected reward. We represent the total reward for a given trajectory $\\tau$ as $r(\\tau)$.<br />\n",
    "\n",
    "$$J(\\theta) = {E_{\\pi}}[r(\\tau)]$$ <br />\n",
    "As we've seen before the expected reward is the value of the start state $s_{0}$ under a policy $\\pi_{\\theta}$.<br />\n",
    "Therefore we have:<br />\n",
    "$$J(\\theta) = v_{\\pi_{\\theta}}(s_{0}) = {E_{\\pi}}[r(\\tau)]$$\n",
    "\n",
    "The equations look pretty cool, but what next?<br />\n",
    "We can derive some intuition from the loss functions used in machine learning. A loss function is defined with respect to the parameters $\\theta$ and we use **gradient descent** to find the parameters $\\theta$ that minimize the loss. \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\alpha \\nabla L(\\theta_{t})$$\n",
    "\n",
    "In Reinforcement Learning, however, we want to maximize the expected reward, so what do we do? Well, pretty simple, we go up instead of down, i.e., **gradient ascent** instead of gradient descent.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\nabla J(\\theta_{t})$$\n",
    ">*When the solution is simple, God is answering. - Albert Einstein*\n",
    "\n",
    "We're going to use gradient ascent to maximize our expected reward, but before we can do that, we need to define the following derivative $\\nabla J(\\theta_{t})$.\n",
    "Now we can derive this, but we're going to save ourselves some time and present the answer:\n",
    "\n",
    "$$\\nabla J(\\theta_{t}) = \\nabla_{\\theta} \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi}(s, a)\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta_{t}) \\propto \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "\n",
    "This can be proved mathematically, but for now, we're just going to believe that we have the right answer.<br />\n",
    "Our gradient update can now be written as:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\nabla J(\\theta_{t})$$\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "<img src=\"images/ascent.jpg\" />\n",
    "*There you are, left alone, lost in a mountain range surrounded by snow. What do you do? You decide to get to the highest point and light up a flare so that someone can come to rescue you. Turns out what you are going to do is gradient ascent. Look as far as you can in every direction and find the direction that gets you the highest. Go in that direction.*\n",
    "\n",
    "\n",
    "Now that we have defined our gradient update let's take a look at an example to solidify our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
