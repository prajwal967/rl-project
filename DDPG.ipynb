{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP DETERMINISTIC POLICY GRADIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into this, lets just walk thorugh some basics\n",
    "Polcy gradients methods are a new class of Reinforcement Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put a better intro here - maybe add a brief intro about rl in general- \n",
    "#wjhat a policy is, what state, actions and reward are\n",
    "#and we can also say this is helpful reading before continuing to policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Definition\n",
    "Policy gradient methods learn a parameterized policy that can select actions without consulting a value function. They model and optimize the policy directly.\n",
    "$$\\pi(a|s, \\theta)=Pr\\{A_{t} = a\\ | S_{t} = s, \\theta_{t} = \\theta\\}$$\n",
    "\n",
    "This equation denotes the probability that action a is taken at time t given that the environment is in state s at time t with parameter $\\theta$.\n",
    "\n",
    "An example of a policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Reminds you off the softmax equation we so often see in machine learning doesn't it?\n",
    "We call this kind of policy paramterization as *softmax in action preferences*.\n",
    "Just like in machine learnning we would have $h(s, a, \\theta) = \\theta^T x(s, a)$ where $x(s, a)$ is a feature vector and $\\theta$ are the weights.\n",
    "\n",
    "Now that we covered the **Policy** portion, let's move on to the **Gradient** portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not sure how the feature vector is computed - will need to refer to chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Policy Gradient Theorem\n",
    "Let's define our expected reward. We represent the total reward for a given trajectory $\\tau$ as $r(\\tau)$.\n",
    "$$J(\\theta) = \\mathbb{E_{\\pi}}[r(\\tau)]$$\n",
    "As we'vew seen before the expected reward is the value of the start state $s_{0}$ under a policy $\\pi_{\\theta}$.\\\n",
    "Therefore we have:\n",
    "$$J(\\theta) = v_{\\pi_{\\theta}}(s_{0}) = \\mathbb{E_{\\pi}}[r(\\tau)]$$\n",
    "\n",
    "The equations look pretty cool, but what next? \n",
    "We can derive some intuition from the loss functions used in machine learning. A loss function is defined with respect to the parameters $\\theta$ and we use **gradient descent** to find the paramters $\\theta$ that minimize the loss. \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\alpha \\nabla L(\\theta_{t})$$\n",
    "\n",
    "In Reinforcement Learning however we want maximize the expected reward, so what do we do? Well pretty simple we go up instead of down, i.e **gradient ascent** instead of gradient descent.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\nabla J(\\theta_{t})$$\n",
    ">*\\\"When the solution is simple, God is answering.\\\" - Albert Einstein*\n",
    "\n",
    "We're going to use gradient ascent to maximize our expected reward, but before we can do that we need to define the following derivative $\\nabla J(\\theta_{t})$.\n",
    "Now we can ask you to derive this, but we're going to save you some time and give you the answer:\n",
    "$$\\nabla J(\\theta_{t}) = \\nabla_{\\theta} \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi}(s, a)\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta_{t}) \\propto \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "We can prove this mathematicallly, but for now you're going to have to trust us. \\\n",
    "Now that we have defined our gradient update, let's take a look at an example to solidify our understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Example\n",
    "\n",
    "Lets consider an MDP with a single state $s$ and three actions $a_{1}, a_{2}, a{_3}$. Lets assume we start off with an approximate q_value function $q(s, a)$.\\\n",
    "Since we have only one state we can ignore the $\\sum_{s}\\mu(s)$ term.\\\n",
    "Let's use the *softmax in action preferences* policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "So out gradient update will be:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{a}q_{\\pi}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "We have $q(s, a1) = 10, q(s, a2) = 5, q(s, a3) = 2.5$\\\n",
    "Let's assume that $a1$ is our optimal action in state $s$. A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a little more and explain points about the theorem - what u(s) is etc\n",
    "#Basically try and explain the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<img src=\"pg-def.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7224578170133801"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import random\n",
    "gym.logger.set_level(gym.logger.ERROR)\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from builtins import super\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = widgets.FloatSlider(min=0.0, max=1.0, value=1.0, step=0.010)\n",
    "b = widgets.FloatSlider(min=0.0, max=1.0, value=1.0, step=0.005)\n",
    "c = widgets.FloatSlider(min=0.0, max=1.0, value=1.0, step=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliders = [a, b, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FloatSlider(value=1.0, max=1.0, step=0.01),\n",
       " FloatSlider(value=1.0, max=1.0, step=0.005),\n",
       " FloatSlider(value=1.0, max=1.0, step=0.0025)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=[0.0]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FloatSlider(value=1.0, max=1.0, step=0.01)\n",
      "FloatSlider(value=1.0, max=1.0, step=0.005)\n",
      "FloatSlider(value=1.0, max=1.0, step=0.0025)\n"
     ]
    }
   ],
   "source": [
    "for i, s in zip(init, sliders):\n",
    "    print(s)\n",
    "    s.logit = i\n",
    "    s.q_val = s.step * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliders[0].q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliders[1].logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "\n",
    "def update_known_optimal(actions):\n",
    "    # Each action has a .logit attribute that we must update. It also has a .grad attribute\n",
    "    # that gives the gradient of the that action.\n",
    "    actions[0].logit += lr * actions[0].grad\n",
    "    # We return the index of the action we updated and the size of the arrow to draw\n",
    "    # (which should be the size of our update, divided by a scaling factor to make the \n",
    "    # visual more pleasant)\n",
    "    return 0, lr / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in zip(init, sliders):\n",
    "        s.logit = i\n",
    "        s.q_val = s.step * 1000\n",
    "    \n",
    "    def update_values():\n",
    "        exps = [np.exp(e.logit) for e in sliders]\n",
    "        for ex, slid in zip(exps, sliders):\n",
    "            slid.value = ex / np.sum(exps)\n",
    "            slid.grad = slid.value * (1-slid.value)\n",
    "            \n",
    "    update_values()\n",
    "    \n",
    "    def f(a_val10, b_val8, c_val7):\n",
    "        pass\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    it = 0\n",
    "    animation_data = []\n",
    "    while all([v.value < 0.95 for v in [a, b, c]]):\n",
    "        incr_idx, incr_size = update_fn(sliders)\n",
    "        update_values()\n",
    "        it += 1\n",
    "        animation_data.append((incr_idx, incr_size, [v.value for v in sliders]))\n",
    "        \n",
    "    if filename is not None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        def plot_animation(i):\n",
    "            plt.clf()\n",
    "            incr_idx, incr_size, values = animation_data[i]\n",
    "            plt.bar(['Val = %s' % (v.step * 1000) for v in sliders], values, width=0.5)\n",
    "            plt.ylim(0, 1.19)\n",
    "            if incr_size > 0:\n",
    "                plt.annotate('',\n",
    "                    xy=(incr_idx, values[incr_idx] + incr_size + 0.05), xycoords='data',\n",
    "                    xytext=(incr_idx, values[incr_idx]), textcoords='data',\n",
    "                    arrowprops=dict(width=5, connectionstyle=\"arc3\", color='green'),\n",
    "                )\n",
    "            else:\n",
    "                plt.annotate('',\n",
    "                    xy=(incr_idx, values[incr_idx]), xycoords='data',\n",
    "                    xytext=(incr_idx, values[incr_idx] - incr_size + 0.05), textcoords='data',\n",
    "                    arrowprops=dict(width=5, connectionstyle=\"arc3\", color='red'),\n",
    "                )\n",
    "            return fig,\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, plot_animation, frames=list(range(0, len(animation_data))), blit=False)\n",
    "        ani.save(filename, writer='imagemagick', fps=10)\n",
    "        plt.close()\n",
    "        display(Image(filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is natural to expect policy-based methods are more useful in the continuous space. Because there is an infinite number of actions and (or) states to estimate the values for and hence value-based approaches are way too expensive computationally in the continuous space. For example, in generalized policy iteration, the policy improvement step argmaxa∈Qπ(s,a)\n",
    "arg\n",
    "⁡\n",
    "max\n",
    "a\n",
    "∈\n",
    "A\n",
    "Q\n",
    "π\n",
    "(\n",
    "s\n",
    ",\n",
    "a\n",
    ")\n",
    " requires a full scan of the action space, suffering from the curse of dimensionality.\n",
    "\n",
    "Using gradient ascent, we can move θ\n",
    "θ\n",
    " toward the direction suggested by the gradient ∇θJ(θ)\n",
    "∇\n",
    "θ\n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    " to find the best θ\n",
    "θ\n",
    " for πθ\n",
    "π\n",
    "θ\n",
    " that produces the highest return."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
