{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP DETERMINISTIC POLICY GRADIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RL agent interacts with its environment, takes an action, and learns more about the environment, influencing subsequent actions and learning. Like an RL agent, we can think of the process of learning about deep deterministic policy gradients as navigating an enviroment consisiting of 3 states. The start state being the policy gradient state. Once we have a basic understanding of the policy gradient state, we take an action which leads us to the Deterministic Policy Gradients state. Finally we take the action that leads us to our goal state, i.e., Deep Deterministic Policy Gradients.\n",
    "\n",
    "**Let's head to our start state - Policy Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Definition\n",
    "Policy gradient methods learn a parameterized policy that can select actions without consulting a value function. They model and optimize the policy directly.\n",
    "\n",
    "$$\\pi(a|s, \\theta)=Pr\\{A_{t} = a\\ | S_{t} = s, \\theta_{t} = \\theta\\}$$\n",
    "\n",
    "This equation denotes the probability that action a is taken at time t given that the environment is in state s at time t with parameter $\\theta$.\n",
    "\n",
    "An example of a policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Reminds you of the softmax equation we so often see in machine learning, doesn't it?\n",
    "We call this kind of policy parameterization as *softmax in action preferences*.\n",
    "Like in machine learning we would have $h(s, a, \\theta) = \\theta^T x(s, a)$ where $x(s, a)$ is a feature vector and $\\theta$ are the weights.\n",
    "\n",
    "**Now that we've covered the *Policy* portion, let's move on to the *Gradient* portion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Policy Gradient Theorem\n",
    "Let's define our expected reward. We represent the total reward for a given trajectory $\\tau$ as $r(\\tau)$.<br />\n",
    "\n",
    "$$J(\\theta) = {E_{\\pi}}[r(\\tau)]$$ <br />\n",
    "As we've seen before the expected reward is the value of the start state $s_{0}$ under a policy $\\pi_{\\theta}$.<br />\n",
    "Therefore we have:<br />\n",
    "$$J(\\theta) = v_{\\pi_{\\theta}}(s_{0}) = {E_{\\pi}}[r(\\tau)]$$\n",
    "\n",
    "The equations look pretty cool, but what next?<br />\n",
    "We can derive some intuition from the loss functions used in machine learning. A loss function is defined with respect to the parameters $\\theta$ and we use **gradient descent** to find the parameters $\\theta$ that minimize the loss. \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\alpha \\nabla L(\\theta_{t})$$\n",
    "\n",
    "In Reinforcement Learning, however, we want to maximize the expected reward, so what do we do? Well, pretty simple, we go up instead of down, i.e., **gradient ascent** instead of gradient descent.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\nabla J(\\theta_{t})$$\n",
    ">*When the solution is simple, God is answering. - Albert Einstein*\n",
    "\n",
    "We're going to use gradient ascent to maximize our expected reward, but before we can do that, we need to define the following derivative $\\nabla J(\\theta_{t})$.\n",
    "Now we can derive this, but we're going to save ourselves some time and present the answer:\n",
    "\n",
    "$$\\nabla J(\\theta_{t}) = \\nabla_{\\theta} \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta_{t}) \\propto \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "\n",
    "This can be proved mathematically, but for now, we're just going to believe that we have the right answer.<br />\n",
    "Our gradient update can now be written as:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\nabla J(\\theta_{t})$$\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "<img src=\"images/ascent.jpg\" />\n",
    "*There you are, left alone, lost in a mountain range surrounded by snow. What do you do? You decide to get to the highest point and light up a flare so that someone can come to rescue you. Turns out what you are going to do is gradient ascent. Look as far as you can in every direction and find the direction that gets you the highest. Go in that direction.*\n",
    "\n",
    "\n",
    "Now that we have defined our gradient update let's take a look at an example to solidify our understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Example\n",
    "\n",
    "Lets consider an MDP with a single state $s$ and three actions $a_{1}, a_{2}, a{_3}$. Lets assume we start off with an approximate q_value function $q(s, a)$.\\\n",
    "Since we have only one state we can ignore the $\\sum_{s}\\mu(s)$ term.\\\n",
    "Let's use the *softmax in action preferences* policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "So out gradient update will be:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "Let $a1$ be the optimal action in state $s$.\n",
    "\n",
    "If we apply policy gradient to this example, we would want\n",
    "\\begin{equation*}\n",
    "\\pi(a1|s, \\theta) = \\frac{e^{h(s, a1, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}} \\approx 1\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi(a2|s, \\theta) = \\frac{e^{h(s, a2, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}} \\approx 0\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi(a3|s, \\theta) = \\frac{e^{h(s, a3, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}} \\approx 0\n",
    "\\end{equation*}\n",
    "\n",
    "As seen in the example below, as the agent interacts with the environment it performs a gradient update. It updates the parameters $\\theta$ to maximize the reward $J(\\theta)$. The parameter updates take a step in the direction of the expected reward. The updates should increase the weights $\\theta$ for features $x(s,a1)$ corresponding to action $a1$ and decrease the weights $\\theta$ for features $x(s,a2)$ and $x(s,a3)$ corresponding to actions $a2$ and $a3$.\n",
    "\n",
    "As the agent continues to interact with the environment, gradient ascent updates the parameters $\\theta$ which increases the value of $h(s,a1,\\theta)$ and decreases the value of $h(s,a2,\\theta)$ and $h(s,a3,\\theta)$. This leads to an increase in the softmax probability of action $a1$ and decrease the softmax probability of actions $a2$ and $a3$. The probability of action $a1$ gets closer to 1 while the probability of actions $a2, a3$ get closer to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/random_action.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Policy Gradient Algorithms\n",
    "Now that we have a general idea of the intuition behind policy gradients, we can derive policy gradient learning algorithms. We won't go into too much detail, but rather briefly explain how each learning algorithm estimates the parameters $\\theta$ for the policy $\\pi(a|s, \\theta)$.\n",
    "\n",
    "The following equation forms the basis of the learning algorithms:\n",
    "$$\\nabla J(\\theta) = \\nabla_{\\theta} \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta) \\propto \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\sum_{a}q_{\\pi_{\\theta}}(S_{t}, a)\\nabla_{\\theta}\\pi_{\\theta}(a|S_{t},\\theta)]$$\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{a}\\hat q(S_{t}, a, w)\\nabla_{\\theta}\\pi_{\\theta}(a|S_{t},\\theta)$$\n",
    "\n",
    "The algorithms that are based on updates that involve all of the actions are called all-action methods. We leave that out for now and we study the classical **REINFORCE** algorithm, whose update at time $t$ involves just a single action $A_{t}$ taken at time $t$ by the policy. Since we are using an action $A_{t}$ taken at time t by the policy, this could lead to a problem where the algorithm happens to update actions that have higher values of $\\pi_{\\theta}$ more often. Unfortunately this can be affected by how $\\pi_{\\theta}$ has been initialized. To avoid this we need to perform on-policy correction, where we need to compensate for the fact that the more probable actions are going to be updated more often. To correct this we weight the term by $\\pi_{\\theta}(a|s,\\theta)$, so if an action $A_{t}$ is sampled $X$ times more often than the other actions, we are going to have $X$ times more updates, however each update will be $X$ times smaller.\n",
    "\n",
    "On policy correction results in the following equations:\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\sum_{a}\\pi_{\\theta}(a|S_{t},\\theta)q_{\\pi_{\\theta}}(S_{t}, a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a|S_{t},\\theta)}{\\pi_{\\theta}(a|S_{t},\\theta)}]$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[q_{\\pi_{\\theta}}(S_{t}, A_{t})\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "\n",
    "Keeping this in mind we move on to our first Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 REINFORCE: Monte Carlo Policy Gradient\n",
    "As the name mentions we use the the monte carlo method to estimate $q_{\\pi_{\\theta}}(S_{t}, A_{t})$. We calculate the expected return $G_{t}$. So the equation for the policy gradient is:\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[G_{t}\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "The parameter update is:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha G_{t}\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}$$\n",
    "\n",
    "<img src=\"images/reinforce-mc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 REINFORCE with Baseline\n",
    "The policy gradient theorem can bbe generalized to include a comparison of the action value to an arbitary baseline $b(s)$. You can think of $b(s)$ as a true label.\n",
    "\n",
    "$$\\nabla J(\\theta) \\propto \\sum_{s}\\mu(s)\\sum_{a}(q_{\\pi_{\\theta}}(s, a)-b(s))\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "\n",
    "Including this baseline does nnot affect the gradient, as shown below:\n",
    "\n",
    "$$\\sum_{a}b(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta) = b(s)\\nabla_{\\theta}\\sum_{a}\\pi_{\\theta}(a|s,\\theta) = b(s)\\nabla_{\\theta}1 = 0$$\n",
    "\n",
    "The baseline $b(s)$ can be any function, even a random variable, as long as it does not vary with $a$.\n",
    "\n",
    "The equation for the policy gradient is:\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[(G_{t}-b(S_{t}))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "The parameter update is:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha (G_{t}-b(S_{t}))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}$$\n",
    "\n",
    "A natural choice for the baseline is an estimate of the state value $\\hat v(S_{t}, w)$, where $w$ is a weight vector that is learned. Baselines don't affect the expected value of the update, but can reduce the variance, which speeds up the learning. In summary, a baseline will reduce the variance while keeping the gradient unbiased.\n",
    "\n",
    "<img src=\"images/reinforce-baseline.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Actor-Critic Methods\n",
    "In these methods the \"actor\" is a reference to the learned policy, and the \"critic\" refers to the learned value function. Although the REINFORCE with Baseline learns both a policy and a state value function, it is not considered an actor-critic method because the learned state value function is used as a baseline and not a critic. For the state value function to be a critic we need to include it as a bootstrapped estimate of the expected return.\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[(G_{t:t+1}-\\hat v(S_{t}, w))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[(R_{t+1}+\\gamma\\hat v(S_{t+1}, w)-\\hat v(S_{t}, w))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "The parameter update is:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha (R_{t+1}+\\gamma\\hat v(S_{t+1}, w)-\\hat v(S_{t}, w))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}$$\n",
    "\n",
    "Since we use bootstrapping, we introduce bias - an asymptotic dependence on the quality of the function approximation. The introduction of bias reduces variance and speeds up the learning. The REINFORCE methods presented in the earlier sections are unbiased, but the speed of learning is much slower. With actor-critic methods we also do not need to wait till the end of an episode to make an update unlike the REINFORCE methods.\n",
    "\n",
    "<img src=\"images/actor-critic.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Why Policy Gradients\n",
    "We've seen how policy gradients work, but we haven't yet convinced you on why you would want to use them. We're going to try and do that now. We're going to present some advantages and that should give you an intuition on why you may want to use policy gradient algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.1 Convergence\n",
    "* Policy based methods have better convergence properties.\n",
    "* The problem with value based methods is the choice of action may vary for a small change in the estimated Q values. Since the action can vary for small changes, the value functions tend to vary in every iteration.\n",
    "* In Policy Gradient, we use gradient ascent. We move the $\\theta$ values in the direction of the gradient $\\nabla_{\\theta}J(\\theta)$. Since we follow the gradient, we're guaranteed to converge to a local maximum (worst case) or a global maximum (best case).\n",
    "\n",
    "<img src=\"images/converge.jpg\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.2 High Dimensional Action Spaces\n",
    "* Policy gradients are more useful in continuing tasks or tasks that contain a very large action space.\n",
    "* The problem with GPI algorithms, is the action predictions are based on the maximum expected reward for each possible action, at each time step, given the current state. This requires a search of the entire action space and this is what we call the curse of dimensionality.\n",
    "* Policy gradients, on the other hand, would just require a feature vector h(s, a, $\\theta$), a set of adjustable parameters, to perform the softmax computation and pick an action. \n",
    "\n",
    "<img src=\"images/highaction.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.3 Stochastic Policies\n",
    "* Policy gradients can learn stochastic policies. We've already seen an example of a stochastic policy, the softmax policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "* Value based methods are unable to learn stochastic policies.\n",
    "* An advantage of stochastic policies is we don't need to have an exploitation v/s exploration trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Disadvantages\n",
    "#### 1.4.2.1 Convergence\n",
    "* An obvious disadvantage is that the gradient ascent can converge to local maximum instead of the global maixmum, which would yield a suboptimal policy.\n",
    "\n",
    "<img src=\"./images/ascentdrift.jpg\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have a better understanding of the policy gradients, we can take the action that takes us from the policy gradient state to the Deterministic Policy Gradients state**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Definition\n",
    "In the policy gradients discussed above, the policy $\\pi(a|s, \\theta)$ is a probability distribution over the actions given the current state, i.e., it is a stochastic policy. **Deterministic policy gradient (DPG)** instead learns a parameterized policy that selects actions as a deterministic decision.\n",
    "\n",
    "$$a = \\pi_{\\theta}(s, \\theta)$$\n",
    "\n",
    "This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. An obvious advantage from a practical standpoint is that a stochastic policy gradient would require more samples to estimate as compared to a deterministic policy gradient. This is because in the stochastic policy gradient we have to compute the sum over both the state and action spaces, whereas in the deterministic policy gradient we only sum over the state space. This presents a huge advantage in environments where the dimension of the action space is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Deterministic Policy Gradient Theorem\n",
    "Taking inspiration from the policy gradient theorem presented above, we can define the objective for deterministic policy gradient (deterministic analogue to the policy gradient theorem) as follows:\n",
    "\n",
    "$$J(\\theta) = \\sum_{s}\\mu(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$J(\\theta) = {E_{\\pi_{\\theta}}}[q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))]$$\n",
    "\n",
    "$$\\nabla J(\\theta) = \\nabla_{\\theta} \\sum_{s}\\mu(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$a=\\pi_{\\theta}(s,\\theta))$$\n",
    "\n",
    "$$\\nabla J(\\theta) = \\sum_{s}\\mu(s)\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "We could apply this objective to one of the methods discussed above like actor-critic. This would give us a deterministic on-policy actor-critic algorithm.\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\nabla_{a}G_{t:t+1}\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\nabla_{a}\\hat q_{\\pi_{\\theta}}(s, a, w)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "We subbstitue a differentiable action value function $\\hat q_{\\pi_{\\theta}}(s, a, w)$ in place of the true action value function $q_{\\pi_{\\theta}}(s, a)$. The parameter updates in the on-policy actor critic method would involve the following steps:\n",
    "\n",
    "$$A_{t}=\\pi_{\\theta}(S_{t},\\theta)), A_{t+1}=\\pi_{\\theta}(S_{t+1},\\theta))$$\n",
    "\n",
    "$$\\delta_{t} = R_{t+1} + \\gamma\\hat q_{\\pi_{\\theta}}(S_{t+1}, A_{t+1}, w) - \\hat q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}\\hat q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}\\nabla_{a}\\hat q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)\\nabla_{\\theta}\\pi_{\\theta}(S_{t},\\theta)$$\n",
    "\n",
    "The problem with an on policty method in deterministic policy gradients is the absence of exploration. Since the policy is deterministic, it is not going to place an emphasis on exploration and would rather exploit the actions selected deterministically. To explore the state and action space we can use an off-policy learning algorithm where the behaviour policy is a stochastic policy (encourages exploration) and the target policy is the deterministic policy.\n",
    "\n",
    "<img src=\"images/off-policy.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Off-Policy Deterministic Actor-Critic\n",
    "We use off-policy learning to learn a deterministic target policy $\\pi_{\\theta}(s, \\theta)$ from sequences generated by an arbitrary stochastic behaviour policy $\\beta_{\\theta}(s, a, \\theta)$.\n",
    "We modify the objective function to represent off-policy learning:\n",
    "\n",
    "$$J(\\theta) = \\sum_{s}\\mu_{\\beta_{\\theta}}(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$J(\\theta) = {E_{\\beta_{\\theta}}}[q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))]$$\n",
    "$\\mu_{\\beta_{\\theta}}$ represents the state distribution under the stochastic behaviour policy $\\beta_{\\theta}(a|s, \\theta)$.\n",
    "\n",
    "$$\\nabla J(\\theta) = \\nabla_{\\theta} \\sum_{s}\\mu_{\\beta_{\\theta}}(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$a=\\pi_{\\theta}(s,\\theta))$$\n",
    "\n",
    "$$\\nabla J(\\theta) = \\sum_{s}\\mu_{\\beta_{\\theta}}(s)\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)$$\n",
    "$$\\nabla J(\\theta) = {E_{\\beta_{\\theta}}}[\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "We can derive an update rule similiar to the on policy case. We subbstitue a differentiable action value function $\\hat q_{\\pi_{\\theta}}(s, a, w)$ in place of the true action value function $q_{\\pi_{\\theta}}(s, a)$. The parameter updates in the off-policy actor critic method would involve the following steps:\n",
    "\n",
    "$$A_{t}=\\beta_{\\theta}(\\bullet|S_{t},\\theta)), A_{t+1}=\\pi_{\\theta}(S_{t+1},\\theta))$$\n",
    "\n",
    "$$\\delta_{t} = R_{t+1} + \\gamma\\hat q_{\\pi_{\\theta}}(S_{t+1}, A_{t+1}, w) - \\hat q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}\\hat q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}\\nabla_{a}\\hat q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)\\nabla_{\\theta}\\pi_{\\theta}(S_{t},\\theta)$$\n",
    "\n",
    "Two key points to note are:\n",
    "1. The deterministic policy gradient removes the summation over the actions, therefore we don't need to perform off-policy correction (importance sampling) for the actor.\n",
    "2. The algorithm presented above uses Q-learning, which avoids having to include importance sampling in the critc.\n",
    "\n",
    "The deterministic policy gradient theorem can be plugged into common policy gradient frameworks presented in the previous sections. \n",
    "\n",
    "<img src=\"images/dpg-code.png\" />\n",
    "\n",
    "**PLOT graphs from the paper? - comparing PG and DPG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients can be estimated more efficiently than their stochastic counterparts, avoiding a problematic integral over the action space. In practice, the deterministic actor-critic significantly outperformed its stochastic counterpart by several orders of magnitude in a bandit with 50 continuous action dimensions, and solved a challenging reinforcement learning problem with 20 con- tinuous action dimensions and 50 state dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Definition\n",
    "**Deep Deterministic Policy Gradients (DDPG)** is an actor-critic, model-free algorithm based on the **Deep Q Network (DQN)** and **Deterministic Policy Gradient (DPG)** that can operate over continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Motivation\n",
    "You can think of Deep Deterministic Policy Gradients (DDPG) as a solution to the problems of Deep Q Networks (DQN) and Deterministic Policy Gradients (DPG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, while DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control tasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straight- forwardly applied to continuous domains since it relies on a finding the action that maximizes the action-value function, which in the continuous valued case requires an iterative optimization process at every step.\n",
    "\n",
    "on the deterministic policy gradient (DPG) algorithm (Silver et al., 2014) (itself similar to NFQCA (Hafner & Riedmiller, 2011), and similar ideas can be found in (Prokhorov et al., 1997)). However, as we show below, a naive application of this actor-critic method with neural function approximators is unstable for challenging problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2 Definition\n",
    "**Deep Deterministic Policy Gradients (DDPG)** are is a d\n",
    "\n",
    "\n",
    "\n",
    "adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
