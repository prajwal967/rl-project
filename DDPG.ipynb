{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP DETERMINISTIC POLICY GRADIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RL agent interacts with its environment, takes an action, and learns more about the environment, influencing subsequent actions and learning. Like an RL agent, we can think of the process of learning about deep deterministic policy gradients as navigating an enviroment consisiting of 3 states. The start state being the policy gradient state. Once we have a basic understanding of the policy gradient state, we take an action which leads us to the Deterministic Policy Gradients state. Finally we take the action that leads us to our goal state, i.e., Deep Deterministic Policy Gradients.\n",
    "\n",
    "**Let's head to our start state - Policy Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Definition\n",
    "Policy gradient methods learn a parameterized policy that can select actions without consulting a value function. They model and optimize the policy directly.\n",
    "\n",
    "$$\\pi(a|s, \\theta)=Pr\\{A_{t} = a\\ | S_{t} = s, \\theta_{t} = \\theta\\}$$\n",
    "\n",
    "This equation denotes the probability that action a is taken at time t given that the environment is in state s at time t with parameter $\\theta$.\n",
    "\n",
    "An example of a policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Reminds you of the softmax equation we so often see in machine learning, doesn't it?\n",
    "We call this kind of policy parameterization as *softmax in action preferences*.\n",
    "Like in machine learning we would have $h(s, a, \\theta) = \\theta^T x(s, a)$ where $x(s, a)$ is a feature vector and $\\theta$ are the weights.\n",
    "\n",
    "**Now that we've covered the *Policy* portion, let's move on to the *Gradient* portion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Policy Gradient Theorem\n",
    "Let's define our expected reward. We represent the total reward for a given trajectory $\\tau$ as $r(\\tau)$.<br />\n",
    "\n",
    "$$J(\\theta) = {E_{\\pi}}[r(\\tau)]$$ <br />\n",
    "As we've seen before the expected reward is the value of the start state $s_{0}$ under a policy $\\pi_{\\theta}$.<br />\n",
    "Therefore we have:<br />\n",
    "$$J(\\theta) = v_{\\pi_{\\theta}}(s_{0}) = {E_{\\pi}}[r(\\tau)]$$\n",
    "\n",
    "The equations look pretty cool, but what next?<br />\n",
    "We can derive some intuition from the loss functions used in machine learning. A loss function is defined with respect to the parameters $\\theta$ and we use **gradient descent** to find the parameters $\\theta$ that minimize the loss. \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\alpha \\nabla L(\\theta_{t})$$\n",
    "\n",
    "In Reinforcement Learning, however, we want to maximize the expected reward, so what do we do? Well, pretty simple, we go up instead of down, i.e., **gradient ascent** instead of gradient descent.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\nabla J(\\theta_{t})$$\n",
    ">*When the solution is simple, God is answering. - Albert Einstein*\n",
    "\n",
    "We're going to use gradient ascent to maximize our expected reward, but before we can do that, we need to define the following derivative $\\nabla J(\\theta_{t})$.\n",
    "Now we can derive this, but we're going to save ourselves some time and present the answer:\n",
    "\n",
    "$$\\nabla J(\\theta_{t}) = \\nabla_{\\theta} \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta_{t}) \\propto \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "\n",
    "This can be proved mathematically, but for now, we're just going to believe that we have the right answer.<br />\n",
    "Our gradient update can now be written as:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\nabla J(\\theta_{t})$$\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "<img src=\"images/ascent.jpg\" />\n",
    "*There you are, left alone, lost in a mountain range surrounded by snow. What do you do? You decide to get to the highest point and light up a flare so that someone can come to rescue you. Turns out what you are going to do is gradient ascent. Look as far as you can in every direction and find the direction that gets you the highest. Go in that direction.*\n",
    "\n",
    "\n",
    "Now that we have defined our gradient update let's take a look at an example to solidify our understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Example\n",
    "\n",
    "Lets consider an MDP with a single state $s$ and three actions $a_{1}, a_{2}, a{_3}$. Lets assume we start off with an approximate q_value function $q(s, a)$.\\\n",
    "Since we have only one state we can ignore the $\\sum_{s}\\mu(s)$ term.\\\n",
    "Let's use the *softmax in action preferences* policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "So out gradient update will be:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "Let $a1$ be the optimal action in state $s$.\n",
    "\n",
    "If we apply policy gradient to this example, we would want\n",
    "\\begin{equation*}\n",
    "\\pi(a1|s, \\theta) = \\frac{e^{h(s, a1, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}} \\approx 1\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi(a2|s, \\theta) = \\frac{e^{h(s, a2, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}} \\approx 0\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi(a3|s, \\theta) = \\frac{e^{h(s, a3, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}} \\approx 0\n",
    "\\end{equation*}\n",
    "\n",
    "As seen in the example below, as the agent interacts with the environment it performs a gradient update. It updates the parameters $\\theta$ to maximize the reward $J(\\theta)$. The parameter updates take a step in the direction of the expected reward. The updates should increase the weights $\\theta$ for features $x(s,a1)$ corresponding to action $a1$ and decrease the weights $\\theta$ for features $x(s,a2)$ and $x(s,a3)$ corresponding to actions $a2$ and $a3$.\n",
    "\n",
    "As the agent continues to interact with the environment, gradient ascent updates the parameters $\\theta$ which increases the value of $h(s,a1,\\theta)$ and decreases the value of $h(s,a2,\\theta)$ and $h(s,a3,\\theta)$. This leads to an increase in the softmax probability of action $a1$ and decrease the softmax probability of actions $a2$ and $a3$. The probability of action $a1$ gets closer to 1 while the probability of actions $a2, a3$ get closer to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/random_action.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Policy Gradient Algorithms\n",
    "Now that we have a general idea of the intuition behind policy gradients, we can derive policy gradient learning algorithms. We won't go into too much detail, but rather briefly explain how each learning algorithm estimates the parameters $\\theta$ for the policy $\\pi(a|s, \\theta)$.\n",
    "\n",
    "The following equation forms the basis of the learning algorithms:\n",
    "$$\\nabla J(\\theta) = \\nabla_{\\theta} \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta) \\propto \\sum_{s}\\mu(s)\\sum_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\sum_{a}q_{\\pi_{\\theta}}(S_{t}, a)\\nabla_{\\theta}\\pi_{\\theta}(a|S_{t},\\theta)]$$\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha \\sum_{a}\\hat q(S_{t}, a, w)\\nabla_{\\theta}\\pi_{\\theta}(a|S_{t},\\theta)$$\n",
    "\n",
    "The algorithms that are based on updates that involve all of the actions are called all-action methods. We leave that out for now and we study the classical **REINFORCE** algorithm, whose update at time $t$ involves just a single action $A_{t}$ taken at time $t$ by the policy. Since we are using an action $A_{t}$ taken at time t by the policy, this could lead to a problem where the algorithm happens to update actions that have higher values of $\\pi_{\\theta}$ more often. Unfortunately this can be affected by how $\\pi_{\\theta}$ has been initialized. To avoid this we need to perform on-policy correction, where we need to compensate for the fact that the more probable actions are going to be updated more often. To correct this we weight the term by $\\pi_{\\theta}(a|s,\\theta)$, so if an action $A_{t}$ is sampled $X$ times more often than the other actions, we are going to have $X$ times more updates, however each update will be $X$ times smaller.\n",
    "\n",
    "On policy correction results in the following equations:\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\sum_{a}\\pi_{\\theta}(a|S_{t},\\theta)q_{\\pi_{\\theta}}(S_{t}, a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a|S_{t},\\theta)}{\\pi_{\\theta}(a|S_{t},\\theta)}]$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[q_{\\pi_{\\theta}}(S_{t}, A_{t})\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "\n",
    "Keeping this in mind we move on to our first Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='REINFORCE'></a>\n",
    "\n",
    "### 1.3.1 REINFORCE: Monte Carlo Policy Gradient\n",
    "As the name mentions we use the the monte carlo method to estimate $q_{\\pi_{\\theta}}(S_{t}, A_{t})$. We calculate the expected return $G_{t}$. So the equation for the policy gradient is:\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[G_{t}\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "The parameter update is:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha G_{t}\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}$$\n",
    "\n",
    "<img src=\"images/reinforce-mc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 REINFORCE with Baseline\n",
    "The policy gradient theorem can bbe generalized to include a comparison of the action value to an arbitary baseline $b(s)$. You can think of $b(s)$ as a true label.\n",
    "\n",
    "$$\\nabla J(\\theta) \\propto \\sum_{s}\\mu(s)\\sum_{a}(q_{\\pi_{\\theta}}(s, a)-b(s))\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta)$$\n",
    "\n",
    "Including this baseline does nnot affect the gradient, as shown below:\n",
    "\n",
    "$$\\sum_{a}b(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s,\\theta) = b(s)\\nabla_{\\theta}\\sum_{a}\\pi_{\\theta}(a|s,\\theta) = b(s)\\nabla_{\\theta}1 = 0$$\n",
    "\n",
    "The baseline $b(s)$ can be any function, even a random variable, as long as it does not vary with $a$.\n",
    "\n",
    "The equation for the policy gradient is:\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[(G_{t}-b(S_{t}))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "The parameter update is:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha (G_{t}-b(S_{t}))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}$$\n",
    "\n",
    "A natural choice for the baseline is an estimate of the state value $\\hat v(S_{t}, w)$, where $w$ is a weight vector that is learned. Baselines don't affect the expected value of the update, but can reduce the variance, which speeds up the learning. In summary, a baseline will reduce the variance while keeping the gradient unbiased.\n",
    "\n",
    "<img src=\"images/reinforce-baseline.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Actor-Critic Methods\n",
    "In these methods the \"actor\" is a reference to the learned policy, and the \"critic\" refers to the learned value function. Although the REINFORCE with Baseline learns both a policy and a state value function, it is not considered an actor-critic method because the learned state value function is used as a baseline and not a critic. For the state value function to be a critic we need to include it as a bootstrapped estimate of the expected return.\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[(G_{t:t+1}-\\hat v(S_{t}, w))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[(R_{t+1}+\\gamma\\hat v(S_{t+1}, w)-\\hat v(S_{t}, w))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}]$$\n",
    "The parameter update is:\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha (R_{t+1}+\\gamma\\hat v(S_{t+1}, w)-\\hat v(S_{t}, w))\\frac{\\nabla_{\\theta}\\pi_{\\theta}(A_{t}|S_{t},\\theta)}{\\pi_{\\theta}(A_{t}|S_{t},\\theta)}$$\n",
    "\n",
    "Since we use bootstrapping, we introduce bias - an asymptotic dependence on the quality of the function approximation. The introduction of bias reduces variance and speeds up the learning. The REINFORCE methods presented in the earlier sections are unbiased, but the speed of learning is much slower. With actor-critic methods we also do not need to wait till the end of an episode to make an update unlike the REINFORCE methods.\n",
    "\n",
    "<img src=\"images/actor-critic.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Why Policy Gradients\n",
    "We've seen how policy gradients work, but we haven't yet convinced you on why you would want to use them. We're going to try and do that now. We're going to present some advantages and that should give you an intuition on why you may want to use policy gradient algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.1 Convergence\n",
    "* Policy based methods have better convergence properties.\n",
    "* The problem with value based methods is the choice of action may vary for a small change in the estimated Q values. Since the action can vary for small changes, the value functions tend to vary in every iteration.\n",
    "* In Policy Gradient, we use gradient ascent. We move the $\\theta$ values in the direction of the gradient $\\nabla_{\\theta}J(\\theta)$. Since we follow the gradient, we're guaranteed to converge to a local maximum (worst case) or a global maximum (best case).\n",
    "\n",
    "<img src=\"images/converge.jpg\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.2 High Dimensional Action Spaces\n",
    "* Policy gradients are more useful in continuing tasks or tasks that contain a very large action space.\n",
    "* The problem with GPI algorithms, is the action predictions are based on the maximum expected reward for each possible action, at each time step, given the current state. This requires a search of the entire action space and this is what we call the curse of dimensionality.\n",
    "* Policy gradients, on the other hand, would just require a feature vector h(s, a, $\\theta$), a set of adjustable parameters, to perform the softmax computation and pick an action. \n",
    "\n",
    "<img src=\"images/highaction.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1.3 Stochastic Policies\n",
    "* Policy gradients can learn stochastic policies. We've already seen an example of a stochastic policy, the softmax policy:\n",
    "\\begin{equation*}\n",
    "\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}} {\\sum_{b}{e^{h(s, b, \\theta)}}}\n",
    "\\end{equation*}\n",
    "* Value based methods are unable to learn stochastic policies.\n",
    "* An advantage of stochastic policies is we don't need to have an exploitation v/s exploration trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Disadvantages\n",
    "#### 1.4.2.1 Convergence\n",
    "* An obvious disadvantage is that the gradient ascent can converge to local maximum instead of the global maixmum, which would yield a suboptimal policy.\n",
    "\n",
    "<img src=\"./images/ascentdrift.jpg\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have a better understanding of the policy gradients, we can take the action that takes us from the policy gradient state to the Deterministic Policy Gradients state**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Definition\n",
    "In the policy gradients discussed above, the policy $\\pi(a|s, \\theta)$ is a probability distribution over the actions given the current state, i.e., it is a stochastic policy. **Deterministic policy gradient (DPG)** instead learns a parameterized policy that selects actions as a deterministic decision.\n",
    "\n",
    "$$a = \\pi_{\\theta}(s, \\theta)$$\n",
    "\n",
    "This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. An obvious advantage from a practical standpoint is that a stochastic policy gradient would require more samples to estimate as compared to a deterministic policy gradient. This is because in the stochastic policy gradient we have to compute the sum over both the state and action spaces, whereas in the deterministic policy gradient we only sum over the state space. This presents a huge advantage in environments where the dimension of the action space is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Deterministic Policy Gradient Theorem\n",
    "Taking inspiration from the policy gradient theorem presented above, we can define the objective for deterministic policy gradient (deterministic analogue to the policy gradient theorem) as follows:\n",
    "\n",
    "$$J(\\theta) = \\sum_{s}\\mu(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$J(\\theta) = {E_{\\pi_{\\theta}}}[q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))]$$\n",
    "\n",
    "$$\\nabla J(\\theta) = \\nabla_{\\theta} \\sum_{s}\\mu(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$a=\\pi_{\\theta}(s,\\theta))$$\n",
    "\n",
    "$$\\nabla J(\\theta) = \\sum_{s}\\mu(s)\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)$$\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "We could apply this objective to one of the methods discussed above like actor-critic. This would give us a deterministic on-policy actor-critic algorithm.\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\nabla_{a}G_{t:t+1}\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "$$\\nabla J(\\theta) = {E_{\\pi_{\\theta}}}[\\nabla_{a}\\hat q_{\\pi_{\\theta}}(s, a, w)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "We subbstitue a differentiable action value function $\\hat q_{\\pi_{\\theta}}(s, a, w)$ in place of the true action value function $q_{\\pi_{\\theta}}(s, a)$. The parameter updates in the on-policy actor critic method would involve the following steps:\n",
    "\n",
    "$$A_{t}=\\pi_{\\theta}(S_{t},\\theta)), A_{t+1}=\\pi_{\\theta}(S_{t+1},\\theta))$$\n",
    "\n",
    "$$\\delta_{t} = R_{t+1} + \\gamma\\hat Q_{\\pi_{\\theta}}(S_{t+1}, A_{t+1}, w) - \\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}\\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}\\nabla_{a}\\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)\\nabla_{\\theta}\\pi_{\\theta}(S_{t},\\theta)$$\n",
    "\n",
    "The problem with an on policty method in deterministic policy gradients is the absence of exploration. Since the policy is deterministic, it is not going to place an emphasis on exploration and would rather exploit the actions selected deterministically. To explore the state and action space we can use an off-policy learning algorithm where the behaviour policy is a stochastic policy (encourages exploration) and the target policy is the deterministic policy.\n",
    "\n",
    "<img src=\"images/off-policy.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Off-Policy Deterministic Actor-Critic\n",
    "We use off-policy learning to learn a deterministic target policy $\\pi_{\\theta}(s, \\theta)$ from sequences generated by an arbitrary stochastic behaviour policy $\\beta_{\\theta}(s, a, \\theta)$.\n",
    "We modify the objective function to represent off-policy learning:\n",
    "\n",
    "$$J(\\theta) = \\sum_{s}\\mu_{\\beta_{\\theta}}(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$J(\\theta) = {E_{\\beta_{\\theta}}}[q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))]$$\n",
    "$\\mu_{\\beta_{\\theta}}$ represents the state distribution under the stochastic behaviour policy $\\beta_{\\theta}(a|s, \\theta)$.\n",
    "\n",
    "$$\\nabla J(\\theta) = \\nabla_{\\theta} \\sum_{s}\\mu_{\\beta_{\\theta}}(s)q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s,\\theta))$$\n",
    "$$a=\\pi_{\\theta}(s,\\theta))$$\n",
    "\n",
    "$$\\nabla J(\\theta) = \\sum_{s}\\mu_{\\beta_{\\theta}}(s)\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)$$\n",
    "$$\\nabla J(\\theta) = {E_{\\beta_{\\theta}}}[\\nabla_{a}q_{\\pi_{\\theta}}(s, a)\\nabla_{\\theta}\\pi_{\\theta}(s,\\theta)]$$\n",
    "\n",
    "We can derive an update rule similiar to the on policy case. We substitue a differentiable action value function $\\hat q_{\\pi_{\\theta}}(s, a, w)$ in place of the true action value function $q_{\\pi_{\\theta}}(s, a)$. The parameter updates in the off-policy actor critic method would involve the following steps:\n",
    "\n",
    "$$A_{t}=\\beta_{\\theta}(\\bullet|S_{t},\\theta)), A_{t+1}=\\pi_{\\theta}(S_{t+1},\\theta))$$\n",
    "\n",
    "$$\\delta_{t} = R_{t+1} + \\gamma\\hat Q_{\\pi_{\\theta}}(S_{t+1}, A_{t+1}, w) - \\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}\\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}\\nabla_{a}\\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)\\nabla_{\\theta}\\pi_{\\theta}(S_{t},\\theta)$$\n",
    "\n",
    "Two key points to note are:\n",
    "1. The deterministic policy gradient removes the summation over the actions, therefore we don't need to perform off-policy correction (importance sampling) for the actor.\n",
    "2. The algorithm presented above uses Q-learning, which avoids having to include importance sampling in the critc.\n",
    "\n",
    "The deterministic policy gradient theorem can be plugged into common policy gradient frameworks presented in the previous sections. \n",
    "\n",
    "<img src=\"images/dpg-code.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients can be estimated more efficiently than their stochastic counterparts, avoiding a problematic summation over the action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Definition\n",
    "**Deep Deterministic Policy Gradients (DDPG)** is an actor-critic, model-free algorithm based on the **Deep Q Network (DQN)** and **Deterministic Policy Gradient (DPG)** that can operate over continuous action spaces. DDPG concurrently learns a Q-function and a policy. It uses experience replay and learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Motivation\n",
    "You can think of Deep Deterministic Policy Gradients (DDPG) as a solution to the problems of Deep Q Networks (DQN) and Deterministic Policy Gradients (DPG).\n",
    "\n",
    "<img src=\"images/ddpg-meme.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Deterministic Policy Gradients (DPG)\n",
    "Deterministic policy gradients learn a parameterized policy that selects actions as a deterministic decision.\n",
    "\n",
    "$$a = \\pi_{\\theta}(s, \\theta)$$ \n",
    "\n",
    "**Pros:**\n",
    "* DPG is useful in continuing tasks or tasks that contain a very large action space.\n",
    "* DPG avoids having to loop over the action space\n",
    "\n",
    "**Cons:**\n",
    "* A direct application of neural function approximators in DPG is unstable and causes the learning algorithm to diverge.\n",
    "* Training an actor-critic DPG with temporally-correlated samples introduces a large amount of variance in the Q values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Deep Q Networks (DQN)\n",
    "There has been significant improvements in deep learning approaches used in reinforcement learning. The basis of these approaches is to use a deep neural network function approximator to estimate the action-value function. \n",
    "\n",
    "$$a = \\underset{a}{\\operatorname{argmax}}\\hat Q_{\\pi}(s, a, w)$$ \n",
    "**Pros:**\n",
    "* DQN is able to learn value functions in a stable and robust way using a deep neural network function approximator.\n",
    "* The first innovation of DQN that enables learning the value functions is the use of a replay buffer to pr\n",
    "\n",
    "**Cons:**\n",
    "* For problems that contain high dimensional state and continous action spaces, the DQN approach performs poorly, becauses it involves a max operation over all possible actions. \n",
    "* This max operation is trivial for problems with a finite number of discrete actions, but when the action space is continous, discretizing the action space to exhaustively evaluate the space, and solving the action value optimization is highly non-trivial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Solution (DDPG)\n",
    "What if we took the pros of the DQN algorithm and the pros of the DPG algorithm to negate the effect of the cons of the algorithms? This is exaclty what the authors behind DDPG did.\n",
    "\n",
    "* DDPG selects the actions using a deterministic policy, thereby avoiding the max operation of Q-values over all possible actions.\n",
    "* DDPG adopts the DQN strategy of using a replay buffer and a target Q network to reduce the variance of Q values and improve stability in learning.\n",
    "\n",
    "These two key points ensures that DDPG can work on continous action spaces (select actions using DPG) and reduce variance and improve stability while learning the gradient and Q-value parameters.\n",
    "\n",
    "<img src=\"images/ddpg-def.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Description\n",
    "The Deep Deterministic policy gradient is an off-policy actor critic approach based on deterministic policy gradients.\n",
    "\n",
    "* Actor: $\\pi_{\\theta}(s, \\theta)$\n",
    "* Critic: $\\hat Q_{\\pi_{\\theta}}(s, \\pi_{\\theta}(s, \\theta), w)$\n",
    "\n",
    "As discussed in the section on DPG, the learning updates are as follows:\n",
    "\n",
    "$$A_{t}=\\beta_{\\theta}(\\bullet|S_{t},\\theta)), A_{t+1}=\\pi_{\\theta}(S_{t+1},\\theta))$$\n",
    "\n",
    "$$\\delta_{t} = R_{t+1} + \\gamma\\hat Q_{\\pi_{\\theta}}(S_{t+1}, A_{t+1}, w) - \\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}\\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}\\nabla_{a}\\hat Q_{\\pi_{\\theta}}(S_{t}, A_{t}, w)\\nabla_{\\theta}\\pi_{\\theta}(S_{t},\\theta)$$\n",
    "\n",
    "Unfortunately applying a deep network naively to estimate the parameters of the actor and the critic leads to poor convergence properties. There are two main reasons for this:\n",
    "1. The high variance that arises when estimating action values from temporally-correlated simulated trajectories.\n",
    "2. Instability during learning that arises from the directly updating the actor and critic paramters based on the gradient updates derived from the error signal.\n",
    "\n",
    "To overcome these shortcomings, modifications to DPG were suggested. These modifications were inspired by the success of DQN's. The two key modifications which are discussed further in the next sections are **Experience Replay** and **Target Networks**.\n",
    "* Experience Replay: Store a list of tuples (state, action, reward, next_state). Instead of learning from a recent experience, learn by sampling experiences stored in the list. This approach reduces variance.\n",
    "* Target Networks: As we are learning from estimated targets, we update the target networks slowly. This approach imporves stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Components\n",
    "The DDPG algorithm consists of 6 components:\n",
    "* Q Network\n",
    "* DPG Network\n",
    "* Experience Replay\n",
    "* Target Q Network\n",
    "* Target Policy (DPG) Network\n",
    "* Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.1 Q Network\n",
    "\n",
    "The critic $\\hat Q_{\\pi}(S, A, w)$ is learned using the Bellman equation as in Q-learning. For computing the Q values we use the critic network and pass the action computed by the actor networks ($\\pi(S, \\theta) \\space \\& \\space \\pi'(S,\\theta')$. The Critic loss is a simple TD-error where we use target networks to compute Q-value for the next state. \n",
    "\n",
    "The objective of the critic is to minimize the the Mean-Squared Bellman error (MSBE).\n",
    "\n",
    "<img src=\"images/acq.png\" width=\"670\"/>\n",
    "<img src=\"images/critic.png\" width=\"670\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.2 DPG Network\n",
    "The DPG algorithm maintains a parameterized actor function $\\pi(S, \\theta)$ which specifies the current policy by deterministically mapping states to a specific action. The learned policy $\\pi(S, \\theta)$ will deterministically choose the action that maximizes $\\hat Q_{\\pi}(S, A, w)$.\n",
    "\n",
    "The objective of the actor is to maximize the expected return.\n",
    "\n",
    "<img src=\"images/acd.png\" width=\"670\"/>\n",
    "<img src=\"images/actor.png\" width=\"670\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.3 Experience Replay\n",
    "The optimization algorithms assume that the samples are independent and identically distributed. This assumption is not valid in when samples are generated sequentially. To break up temporal correlation between the samples within training episodes we define a **replay buffer**.\n",
    "\n",
    "The **replay buffer** is a finite sized cache that stores a list of sample **tuples $(s_{t}, a_{t}, r_{t+1}, s_{t+1})$.** This buffer is sampled uniformly to produce a **minibatch of samples**. This minibatch is used to **update the actor and critic (value and policy networks)** as indicated in the image. Since the minibatch is sampled uniformly, it contains samples that are not temporaly correlated, which reduces variance since we learn across a set of uncorrelated transitions.\n",
    "\n",
    "<img src=\"images/replay.png\" width=\"670\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.4 Target Networks (Q & DPG)\n",
    "Directly implementing Q-learning or DPG with neural function approximators is unstable in many environments. This is due to the fact that the Q network ($\\hat Q_{\\pi_{\\theta}}(S, A, w)$) being updated is also being used as a target value for the updates. To overcome this instability we need to have a target values are constrained and robust.\n",
    "\n",
    "The solution is to perform soft target updates. This can be done by creating a copy of the actor ($\\pi'(S,\\theta')$) and critic ($Q'(S, A, w')$) networks. Define the parameters $\\theta'$ and $w'$ close to $\\theta$ and $w$, but with a time delay of sorts. What this means is that the weights of the target networks are updated by having them slowly track the learned weights. As you can see in the code ($\\tau << 1$) only a fraction ($\\tau$) of the main weights ($\\theta, w$) and a fraction ($1-\\tau$) of the target weights ($\\theta', w'$) are transferred to ($\\theta', w'$). Since the weights of the target networks are updated slowly, the target values ($\\pi'(S,\\theta') \\space \\& \\space Q'(S, A, w')$) are constrained and change slowly. The target network delays the propagation of value estimations. This may slow learning, but it greatly improves staibility of learning.\n",
    "\n",
    "<img src=\"images/ac-target.png\" width=\"670\"/>\n",
    "<img src=\"images/target.png\" width=\"670\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.5 Exploration\n",
    "The advantage of having an off-policy algorithm like DDPG is that we can treat exploration as an independent problem. DDPG trains a deterministic policy using an off-policy actor critic. Since the policy is deterministic, the policy would not explore a wide enough variety of actions to find useful learning signals. \n",
    "\n",
    "Since the policy (DPG) is deterministic, we add noise sampled from a noise process $N$ to the actor policy:\n",
    "\n",
    "$$\\pi_{\\theta}(S_{t},\\theta) + N$$\n",
    "\n",
    "The noise process N can be decided based on the environment. The authors of the DDPG paper used Ornstein-Uhlenbeck process to generate temporally correlated exploration for exploration efficiency. It samples noise from a correlated normal distribution. Recent research suggests using an uncorrelated, mean-zero Gaussian noise and reducing the noise over the course of training (reducing the rate of exploration - exploit more).\n",
    "\n",
    "<img src=\"images/exploration.png\" width=\"670\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.6 Pseudocode\n",
    "\n",
    "\n",
    "<img src=\"images/ddpg-algo.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run our experiments on the Open AI LunarLander environment. In this environment, the task is to navigate a lander to its landing pad. \n",
    "\n",
    "Here is a description of the env:\n",
    "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
    "\n",
    "We will deal with the following two versions of this env to test out the different policy gradient algorithms: \n",
    "- Lunar Lander (Discrete)\n",
    "    > Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "- Lunar Lander (Continuous)\n",
    "    > Action is two real values vector from -1 to +1. First controls main engine, -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power. Second value -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off.\n",
    "    \n",
    "<b> For our experiment, we considered an agent to be trained in navigating the lander to the landing pad when the average reward for the last 200 tasks is greater than +200. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving discrete Lunar Lander with REINFORCE policy gradient algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code can be found [here](https://github.com/prajwal967/rl-project/tree/main/code/reinforce). Here is a plot of <i> Rewards </i>(Y-axis) vs <i>No. of episodes </i> (X-axis):\n",
    "\n",
    "<img src=\"code/reinforce/lunar_lander.png\" />\n",
    "\n",
    "We trained the above REINFORCE agent for 1700 episodes. The avg reward for the last 200 episodes plateaued around a reward of 140, and the training was stopped without achieving our experiment target of a reward > 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving continuous Lunar Lander with DDPG policy gradient algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code can be found [here](https://github.com/prajwal967/rl-project/tree/main/code/ddpg). Here is a plot of <i> Rewards </i>(Y-axis) vs <i>No. of episodes </i> (X-axis):\n",
    "\n",
    "<img src=\"code/ddpg/lunar_lander.png\" />\n",
    "\n",
    "The experiment target was successfully achieved by the DDPG agent after running for about ~800 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
